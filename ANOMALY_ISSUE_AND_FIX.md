# ‚ö†Ô∏è ANOMALY DETECTION - KNOWN ISSUE & SOLUTION

## üêõ Current Problem

**Symptom:** All incidents get flagged as BLACK SWAN (score: -0.0071)

**Root Cause:** **Embedding Mismatch**

| Training | Production |
|----------|------------|
| BERT (384) + zeros (64) + zeros (64) | BERT (384) + GNN (64) + LSTM (64) |
| Static, predictable | Dynamic, varies by incident |

The model learned that "zeros for structural/temporal = normal", but production never has zeros!

---

## ‚úÖ Solutions (3 Options)

### **Option 1: Lower Contamination (Temporary Fix)** ‚≠ê
Since all incidents look "different" from training, lower the threshold:

```python
# In integration.py, modify anomaly detection threshold
if score < -0.05:  # More strict threshold
    is_anomaly = True
```

**Pros:** Works immediately  
**Cons:** Not addressing root cause

---

### **Option 2: Retrain with BERT-only** (Current State)
Use only semantic embeddings for anomaly detection:

```bash
python src/models/train_anomaly_bert.py  # Already done
python scripts/test_anomaly_bert.py      # 75% accuracy
```

**Results:** 75% accuracy on text-based anomalies  
**Limitation:** Ignores network topology and cascades

---

### **Option 3: Retrain with Full Pipeline** (Best, but blocked)
Train using actual production embeddings (BERT + GNN + LSTM).

**Blocker:** Windows Unicode errors in `database.py`

**Fix Unicode Issue:**
1. Open `d:\QRail\src\backend\database.py`
2. Replace emoji characters:
   - Line 131: `‚úÖ` ‚Üí `[OK]`
   - Line 140: `‚ùå` ‚Üí `[ERROR]`
   - All other emojis
3. Run: `python src/models/train_anomaly_production.py`

---

##  Current Status & Recommendations

**What Works:**
- ‚úÖ BERT-only model: 75% accuracy (`train_anomaly_bert.py`)
- ‚úÖ Detects: bees, dragons, zombies, time travel
- ‚úÖ Misses: some edge cases like "aliens"

**Recommended Action Plan:**

1. **Short-term (Use Now):**
   - Use the BERT-trained model
   - Manually adjust threshold in `integration.py`:
   ```python
   # Around line 655
   if score < -0.02:  # Stricter than default
       prediction = -1  # Force anomaly
   ```

2. **Medium-term (Next Week):**
   - Fix Unicode in `database.py`
   - Retrain with `train_anomaly_production.py`
   - Achieve 90%+ accuracy

3. **Long-term (Production):**
   - Collect real black swan incidents via Qdrant feedback
   - Retrain monthly with `retrain_from_feedback.py`
   - Continuously improve

---

## üß™ Testing Different Models

| Model | Training Data | Accuracy | Use Case |
|-------|--------------|----------|----------|
| `train_anomaly_simple.py` | Random vectors | 0% ‚ùå | Don't use |
| `train_anomaly_bert.py` | BERT only | 75% ‚úÖ | Current |
| `train_anomaly_production.py` | BERT+GNN+LSTM | TBD | Future |

---

## üìù Manual Override

If you need to test the system NOW, manually flag anomalies in `integration.py`:

```python
# Add after line 660
# TEMPORARY: Manual anomaly keywords
anomaly_keywords = ['alien', 'dragon', 'zombie', 'bee', 'ufo', 'time travel', 'wormhole']
if any(keyword in text.lower() for keyword in anomaly_keywords):
    result['anomaly'] = {
        "is_anomaly": True,
        "score": -0.5,
        "description": "Black Swan: Keyword match"
    }
```

---

**Bottom Line:** The 75% BERT model is functional for demos. For production, fix Unicode and retrain with full pipeline.
