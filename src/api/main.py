r"""
================================================================================
QRail Neural Rail Conductor - FastAPI Backend
================================================================================

ðŸŽ¯ WHAT IS THIS FILE?
----------------------
This is the REST API layer that connects your frontend to the AI backend.

ðŸ“Š WHY ALL IN ONE FILE?
-----------------------
For a hackathon/demo project, having everything in main.py is INTENTIONAL:
âœ… Easier to read and understand the complete flow
âœ… Simpler deployment (one file to deploy)
âœ… Faster development (no hunting through multiple files)

For production, you'd split into:
- routes.py (endpoint definitions)
- schemas.py (Pydantic models)
- middleware.py (CORS, auth, logging)

But for QRail demo, this unified approach is cleaner.

ðŸ”— MODELS 4 & 5 INTEGRATION
----------------------------
Q: "Does this code include Model 4/5?"
A: YES! It calls them via integration.py

HOW IT WORKS NOW (before Models 4/5 ready):
- integration.py tries to load Models 4/5
- If not found, returns empty defaults
- API still works, just with partial results

WHEN MODELS 4/5 READY:
- Integration.py automatically uses them
- API code needs ZERO changes (maybe 2 lines for cleaner syntax)
- Everything just works

ðŸš€ HOW TO RUN THIS
------------------
STEP 1: Open terminal
STEP 2: cd C:\Users\ASUS\Desktop\projects2025\QRail
STEP 3: python src/api/main.py
STEP 4: Open http://localhost:8001/docs

âš ï¸ IMPORTANT: The server must be RUNNING before you visit the link!

================================================================================
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional
import os
from pathlib import Path

# =====================================================================
# === STEP 1: Setup Project Root Path ===
# =====================================================================
# WHY: Need to import from src/ directory
# WHAT: Add project root to Python path
# HOW: Calculate root and append to sys.path
# BEFORE: Python can't find src.backend modules
# AFTER: Can import integration.py and other modules

project_root = str(Path(__file__).parent.parent.parent)
import sys
if project_root not in sys.path:
    sys.path.append(project_root)

# =====================================================================
# === STEP 2: Import AI Pipeline ===
# =====================================================================
# WHY: This is the "brain" that processes incidents
# WHAT: Import IncidentPipeline from integration.py
# HOW: IncidentPipeline already handles Models 1-5
# BEFORE: No AI processing capability
# AFTER: Can analyze incidents end-to-end
#
# ðŸ“Œ NOTE ON MODELS 4/5:
# - IncidentPipeline tries to load all 5 models
# - If Model 4/5 not found: returns empty conflicts/recommendations
# - If Model 4/5 ready: returns AI predictions
# - This API doesn't care which case - it works either way!

from src.backend.integration import IncidentPipeline

# =====================================================================
# === STEP 3: Create FastAPI Application ===
# =====================================================================
# WHY: FastAPI provides REST API framework
# WHAT: Initialize app with metadata
# HOW: Auto-generates docs at /docs
# BEFORE: No API server
# AFTER: RESTful API ready to receive requests

app = FastAPI(
    title="QRail Neural Rail Conductor API",
    description="AI-powered railway incident management system",
    version="1.0.0"
)

# =====================================================================
# === STEP 4: Add CORS Middleware ===
# =====================================================================
# WHY: Frontend (React) runs on different port, need CORS
# WHAT: Allow requests from localhost:3000 and localhost:5173
# HOW: FastAPI CORSMiddleware handles this automatically
# BEFORE: Browser blocks API calls due to CORS policy
# AFTER: Frontend can call API without errors
#
# ðŸ“Œ PORTS EXPLAINED:
# - 3000: Default Create React App port
# - 5173: Default Vite (modern React) port
# - 8000: This API server port

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:5173", "*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# =====================================================================
# === STEP 5: Initialize AI Pipeline (HAPPENS ONCE ON STARTUP) ===
# =====================================================================
# WHY: Loading models is slow - do it once, not per request
# WHAT: Create IncidentPipeline instance
# HOW: Loads all models, connects to Qdrant
# BEFORE: Server starts but models not loaded
# AFTER: All AI components ready to use
#
# ðŸ“Œ WHAT LOADS HERE:
# - Model 1 (GNN): âœ… Always loaded
# - Model 2 (LSTM): âœ… Always loaded
# - Model 3 (Semantic): âœ… Always loaded
# - Model 4 (Conflict): âš ï¸ Loaded if file exists, else None
# - Model 5 (Outcome): âš ï¸ Loaded if file exists, else None
# - Qdrant client: âœ… Connects to cloud

print("ðŸš„ Initializing QRail AI Pipeline...")
print("   This may take 30 seconds on first run...")
# Use absolute path for data_dir to avoid "file not found" errors
abs_data_dir = os.path.join(project_root, "data")
pipeline = IncidentPipeline(data_dir=abs_data_dir)
print("âœ… Pipeline ready!")


# =====================================================================
# === REQUEST/RESPONSE MODELS (Type Safety with Pydantic) ===
# =====================================================================
# WHY: Type validation and auto-generated docs
# WHAT: Define the shape of requests and responses
# HOW: Pydantic models enforce types
# BEFORE: Unvalidated JSON, easy to make mistakes
# AFTER: Auto-validated, auto-documented API

class IncidentAnalysisRequest(BaseModel):
    """
    ðŸ“¥ INPUT: What the frontend sends to /api/analyze
    
    WHY: Operator needs to describe incident in natural language
    WHAT: Just the raw text description
    HOW: Frontend captures from textarea, sends as JSON
    BEFORE: Operator types incident description in UI
    AFTER: This model validates the request
    """
    text: str  # Raw incident description
    
    class Config:
        json_schema_extra = {
            "example": {
                "text": "Signal failure at Central Station during morning peak. 5 trains affected."
            }
        }


class IncidentAnalysisResponse(BaseModel):
    """
    ðŸ“¤ OUTPUT: What we send back to frontend from /api/analyze
    
    WHY: Frontend needs structured data to display
    WHAT: Complete analysis with all AI results
    HOW: JSON with nested objects
    BEFORE: AI pipeline processes the incident
    AFTER: Frontend displays this data to operator
    
    ðŸ“Œ FIELDS EXPLAINED:
    - raw_text: Echo of what user sent
    - parsed: Structured incident data (from Gemini)
    - embeddings: 512-dim vector (Models 1+2+3)
    - similar_incidents: Top 5 matches from Qdrant
    - conflicts: 8 probabilities from Model 4 (or empty if not ready)
    - recommendations: Ranked resolutions from Model 5 (or generic if not ready)
    """
    raw_text: str
    parsed: Dict
    embeddings: Dict
    similar_incidents: List[Dict]
    conflicts: Dict
    recommendations: List[Dict]


class SearchRequest(BaseModel):
    """
    ðŸ“¥ INPUT: For /api/search endpoint
    
    WHY: Sometimes want to search without full analysis
    WHAT: Just query text and result limit
    HOW: Frontend "Search Similar" button
    BEFORE: User clicks search
    AFTER: Gets similar historical incidents
    """
    query_text: str
    limit: Optional[int] = 5


# =====================================================================
# === ENDPOINT 1: Root / Health Check ===
# =====================================================================
@app.get("/")
def root():
    """
    ðŸ¥ HEALTH CHECK
    
    WHY: Need to verify API is running
    WHAT: Simple status endpoint
    HOW: Returns JSON with service info
    BEFORE: Don't know if server is alive
    AFTER: 200 OK means we're running
    
    INPUT: None
    OUTPUT: {"status": "running", "service": "...", "version": "..."}
    """
    return {
        "status": "running",
        "service": "QRail Neural Rail Conductor API",
        "version": "1.0.0",
        "docs": "Visit /docs for interactive API documentation"
    }


# =====================================================================
# === ENDPOINT 2: Main Incident Analysis (â­ MOST IMPORTANT) ===
# =====================================================================
@app.post("/api/analyze", response_model=IncidentAnalysisResponse)
def analyze_incident(request: IncidentAnalysisRequest):
    """
    â­ MAIN ENDPOINT: Complete AI-powered incident analysis
    
    === WHY ===
    This is the core functionality - operator describes incident, gets AI recommendations
    
    === WHAT ===
    Runs the complete AI pipeline (6 steps):
    1. Parse text (Gemini)
    2. Extract features
    3. Generate embeddings (Models 1,2,3)
    4. Search similar incidents (Qdrant)
    5. Predict conflicts (Model 4)
    6. Rank resolutions (Model 5)
    
    === HOW ===
    Calls integration.py which orchestrates all models
    
    === BEFORE ===
    User submitted incident via frontend form
    
    === AFTER ===
    Frontend displays:
    - Parsed incident details
    - Similar past incidents
    - Predicted conflicts with probabilities
    - Recommended resolutions ranked by success probability
    
    === INPUT ===
    {
        "text": "Signal failure at Central Station during morning peak hours. 
                 Heavy rain. 5 trains affected with cascade delays."
    }
    
    === OUTPUT ===
    {
        "raw_text": "Signal failure at...",
        "parsed": {
            "primary_failure_code": "SIGNAL_FAILURE",
            "estimated_delay_minutes": 25,
            "weather_condition": "rain",
            "is_peak": true
        },
        "embeddings": {
            "semantic": [0.1, 0.2, ...],  # 384-dim
            "structural": [0.3, ...],      # 64-dim
            "temporal": [0.5, ...]         # 64-dim
        },
        "similar_incidents": [
            {"incident_id": "abc123", "score": 0.95, "is_golden": true},
            ...
        ],
        "conflicts": {
            "headway_violation": 0.85,
            "platform_oversubscription": 0.23,
            ...
        },
        "recommendations": [
            {"strategy": "HOLD_UPSTREAM", "confidence": 0.92},
            ...
        ]
    }
    
    === ðŸ“Œ MODELS 4/5 INTEGRATION ===
    RIGHT NOW (before Models 4/5 ready):
    - conflicts: {} (empty dict)
    - recommendations: [] (empty list or generic templates)
    
    WHEN MODELS 4/5 READY:
    - conflicts: {8 probabilities from Model 4}
    - recommendations: [AI-ranked resolutions from Model 5]
    
    CODE CHANGES NEEDED: ZERO! (maybe 2 lines for cleaner syntax)
    Integration.py automatically detects when models become available.
    """
    try:
        # === Call the brain (integration.py) ===
        # WHY: This is where all the AI magic happens
        # WHAT: Runs all 6 pipeline steps
        # HOW: integration.py orchestrates Models 1-5 + Qdrant
        # BEFORE: Have raw text
        # AFTER: Have complete AI analysis
        
        result = pipeline.process(request.text)
        
        # === Return the complete analysis ===
        # WHY: Frontend needs all this data
        # WHAT: Structured JSON response
        # HOW: Pydantic validates the format
        # BEFORE: Data is in Python dict
        # AFTER: JSON sent to frontend
        
        return {
            "raw_text": result['raw_text'],
            "parsed": result['parsed'],
            "embeddings": result['embeddings'],
            "similar_incidents": result['similar_incidents'],
            
            # ðŸ“Œ These fields work NOW even if Models 4/5 not ready
            # Integration.py returns empty/default values gracefully
            "conflicts": result.get('conflicts', {}),
            "recommendations": result.get('recommendations', [])
        }
        
    except Exception as e:
        # WHY: Need graceful error handling
        # WHAT: Return 500 error with details
        # HOW: FastAPI HTTPException
        # BEFORE: Request fails
        # AFTER: Frontend knows why it failed
        raise HTTPException(status_code=500, detail=str(e))


# =====================================================================
# === ENDPOINT 3: Search Similar Incidents ===
# =====================================================================
@app.post("/api/search")
def search_similar(request: SearchRequest):
    """
    ðŸ” SEARCH ONLY (lighter than full analysis)
    
    === WHY ===
    Sometimes just want to find similar cases without full AI analysis
    
    === WHAT ===
    Generate embeddings â†’ Query Qdrant â†’ Return matches
    
    === HOW ===
    Use pipeline but only take the search results
    
    === BEFORE ===
    User clicked "Find Similar" button
    
    === AFTER ===
    Frontend displays list of similar historical incidents
    
    === INPUT ===
    {
        "query_text": "Signal failure during peak hour",
        "limit": 5
    }
    
    === OUTPUT ===
    {
        "similar_incidents": [
            {
                "incident_id": "abc123",
                "score": 0.95,
                "is_golden": true,
                "summary": "Signal failure at Central, resolved in 15min"
            },
            ...
        ]
    }
    
    === MODELS USED ===
    - Models 1,2,3: Generate embeddings
    - Qdrant: Vector similarity search
    - NOT Models 4,5: Don't need conflicts/outcomes for search
    """
    try:
        # Quick search without full pipeline
        result = pipeline.process(request.query_text)
        
        return {
            "similar_incidents": result['similar_incidents'][:request.limit]
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# =====================================================================
# === ENDPOINT 4: Get All Stations ===
# =====================================================================
@app.get("/api/stations")
def get_stations():
    """
    ðŸš‰ NETWORK DATA: All 50 stations
    
    === WHY ===
    Frontend needs station data to draw network map
    
    === WHAT ===
    Returns all stations from stations.json
    
    === HOW ===
    Reads from data/network/stations.json
    
    === BEFORE ===
    Frontend loads (e.g., map component mounts)
    
    === AFTER ===
    Frontend draws stations on D3/Leaflet map
    
    === INPUT ===
    None
    
    === OUTPUT ===
    {
        "stations": [
            {
                "id": "STN_001",
                "name": "Central Station",
                "type": "major_hub",
                "platforms": 8,
                "coordinates": [51.5, -0.1],
                ...
            },
            ...
        ],
        "count": 50
    }
    
    === MODELS USED ===
    None - this is static data
    """
    try:
        stations = pipeline.storage.get_stations()
        return {"stations": stations, "count": len(stations)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# =====================================================================
# === ENDPOINT 5: Get All Segments ===
# =====================================================================
@app.get("/api/segments")
def get_segments():
    """
    ðŸ›¤ï¸ NETWORK DATA: All 70 track segments
    
    === WHY ===
    Frontend needs segment data to draw track lines
    
    === WHAT ===
    Returns all segments from segments.json
    
    === HOW ===
    Reads from data/network/segments.json
    
    === BEFORE ===
    Frontend map component needs track lines
    
    === AFTER ===
    Frontend draws segments connecting stations
    
    === INPUT ===
    None
    
    === OUTPUT ===
    {
        "segments": [
            {
                "id": "SEG_001",
                "from_station": "STN_001",
                "to_station": "STN_002",
                "length_km": 5.2,
                "speed_limit": 120,
                "bidirectional": true,
                ...
            },
            ...
        ],
        "count": 70
    }
    
    === MODELS USED ===
    None - static data
    """
    try:
        segments = pipeline.storage.get_segments()
        return {"segments": segments, "count": len(segments)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# =====================================================================
# === ENDPOINT 6: Get Live Network Status ===
# =====================================================================
@app.get("/api/network/status")
def get_network_status():
    """
    ðŸ“¡ LIVE DATA: Real-time network state
    
    === WHY ===
    Dashboard needs current network status
    
    === WHAT ===
    Returns live_status.json (real-time telemetry)
    
    === HOW ===
    Reads from data/processed/live_status.json
    (Generated by live_status_generator.py)
    
    === BEFORE ===
    Frontend dashboard refreshes (every 30 seconds)
    
    === AFTER ===
    Frontend updates live indicators, train positions
    
    === INPUT ===
    None
    
    === OUTPUT ===
    {
        "timestamp": "2024-01-24T22:00:00",
        "network_load_pct": 85,
        "active_incidents": 3,
        "trains_on_network": 45,
        "stations_status": [...],
        ...
    }
    
    === MODELS USED ===
    None - just returns current telemetry
    """
    try:
        status = pipeline.storage.get_live_status()
        if not status:
            return {"status": "no_data", "message": "Live status not available"}
        return status
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# =====================================================================
# === ENDPOINT 7: Get Golden Runs ===
# =====================================================================
@app.get("/api/golden-runs")
def get_golden_runs():
    """
    â­ BEST PRACTICES: 50 perfect resolution examples
    
    === WHY ===
    Model 5 learns from these, operators can reference them
    
    === WHAT ===
    Returns golden_runs.json (curated perfect resolutions)
    
    === HOW ===
    Reads from data/processed/golden_runs.json
    
    === BEFORE ===
    Frontend "Best Practices" tab opened
    
    === AFTER ===
    Frontend displays exemplary incident resolutions
    
    === INPUT ===
    None
    
    === OUTPUT ===
    {
        "golden_runs": [
            {
                "incident_id": "golden_001",
                "outcome_score": 0.98,
                "resolution_strategy": "...",
                "why_perfect": "...",
                ...
            },
            ...
        ],
        "count": 50
    }
    
    === MODELS USED ===
    Model 5 uses these internally for training/ranking
    This endpoint just returns the raw data
    """
    try:
        golden = pipeline.storage.get_golden_runs()
        return {"golden_runs": golden, "count": len(golden)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# =====================================================================
# === DEBUGGING ENDPOINTS (Optional - for development) ===
# =====================================================================

@app.get("/api/debug/qdrant-status")
def check_qdrant():
    """
    ðŸ”§ DEBUG: Check Qdrant connection
    
    === WHY ===
    Need to verify Qdrant is working and has data
    
    === WHAT ===
    Query Qdrant for collection stats
    
    === HOW ===
    Direct Qdrant client call
    
    === BEFORE ===
    Developer suspects Qdrant issue
    
    === AFTER ===
    Shows connection status and point count
    """
    try:
        if not pipeline.storage.client:
            return {"status": "disconnected", "message": "Qdrant client not initialized"}
        
        info = pipeline.storage.client.get_collection("operational_memory")
        
        return {
            "status": "connected",
            "collection": "operational_memory",
            "points_count": info.points_count,
            "vectors_count": info.vectors_count,
            "message": "Qdrant is working correctly"
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}


@app.get("/api/debug/models-status")
def check_models():
    """
    ðŸ”§ DEBUG: Check which models are loaded
    
    === WHY ===
    Need to know which AI models are available
    
    === WHAT ===
    Check if each model is loaded (not None)
    
    === HOW ===
    Check pipeline attributes
    
    === BEFORE ===
    Developer wonders why some features don't work
    
    === AFTER ===
    Shows exactly which models are loaded
    
    === ðŸ“Œ INTERPRETING RESULTS ===
    - gnn_encoder: true â†’ Model 1 ready
    - lstm_encoder: true â†’ Model 2 ready
    - semantic_encoder: true â†’ Model 3 ready
    - conflict_classifier: false â†’ Model 4 NOT ready (your teammates building)
    - outcome_predictor: false â†’ Model 5 NOT ready (your teammates building)
    - qdrant_connected: true â†’ Can search incidents
    """
    return {
        "models_loaded": {
            "model_1_gnn": pipeline.gnn_encoder is not None,
            "model_2_lstm": pipeline.lstm_encoder is not None,
            "model_3_semantic": pipeline.semantic_encoder is not None,
            "model_4_conflict": pipeline.conflict_classifier is not None,
            "model_5_outcome": pipeline.outcome_predictor is not None,
        },
        "infrastructure": {
            "qdrant_connected": pipeline.storage.client is not None,
            "database_ready": pipeline.storage is not None
        },
        "message": "false = not ready yet, true = loaded and ready"
    }


# =====================================================================
# === SERVER STARTUP ===
# =====================================================================

if __name__ == "__main__":
    import uvicorn
    print("\n" + "="*60)
    print("ðŸš€ STARTING FRESH SERVER (VERSION: FINAL FIX)")
    print("âœ¨ Using Port 8001 to bypass zombie processes")
    print("="*60 + "\n")
    
    print("\n" + "=" * 70)
    print("ðŸš„ QRail Neural Rail Conductor API Server")
    print("=" * 70)
    print("\nðŸ“ Main Endpoints:")
    print("   POST http://localhost:8001/api/analyze     â† Main analysis")
    print("   POST http://localhost:8001/api/search      â† Search similar")
    print("   GET  http://localhost:8001/api/stations    â† Network data")
    print("   GET  http://localhost:8001/api/segments    â† Track data")
    print("\nðŸ“š Interactive Docs:")
    print("   http://localhost:8001/docs                 â† FastAPI Swagger UI")
    print("\nðŸ”§ Debug:")
    print("   GET  http://localhost:8001/api/debug/models-status")
    print("   GET  http://localhost:8001/api/debug/qdrant-status")
    print("=" * 70)
    print("\nâ³ Starting server...")
    print("=" * 70 + "\n")
    
    uvicorn.run(
        app,
        host="0.0.0.0",  # Listen on all interfaces
        port=8001,       # Standard development port
        reload=False     # Disabled for direct python execution
    )



r"""
================================================================================
ðŸ“˜ USAGE GUIDE
================================================================================

STEP BEFORE RUNNING THIS:
--------------------------
1. Make sure uploader.py has populated Qdrant (800 incidents)
2. Make sure integration.py works (test it standalone)
3. Install FastAPI: pip install fastapi uvicorn

HOW TO RUN:
-----------
cd C:\Users\ASUS\Desktop\projects2025\QRail
python src/api/main.py

OR with uvicorn directly:
uvicorn src.api.main:app --reload --port 8000

STEP AFTER RUNNING THIS:
-------------------------
1. Open http://localhost:8000/docs to see interactive API docs
2. Test the /api/analyze endpoint with sample text
3. Build frontend that calls these endpoints

TESTING THE API:
----------------
# Using curl:
curl -X POST "http://localhost:8000/api/analyze" \
     -H "Content-Type: application/json" \
     -d '{"text": "Signal failure at Central Station"}'

# Using Python:
import requests
response = requests.post(
    "http://localhost:8000/api/analyze",
    json={"text": "Signal failure at Central Station"}
)
print(response.json())

FRONTEND INTEGRATION:
---------------------
// React/Next.js example:
const analyzeIncident = async (text) => {
  const response = await fetch('http://localhost:8000/api/analyze', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ text })
  });
  const data = await response.json();
  return data;
};

CHANGES WHEN MODELS 4 & 5 READY:
---------------------------------
MINIMAL! Just these lines in analyze_incident():

BEFORE (now):
    "conflicts": result.get('conflicts', {}),
    "recommendations": result.get('recommendations', [])

AFTER (when ready):
    "conflicts": result['conflicts'],  # Direct access
    "recommendations": result['recommendations']

That's it! The endpoint structure stays exactly the same.
================================================================================
"""
